{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d97f4d",
   "metadata": {},
   "source": [
    "# Indonesian Earthquake Markov Chain Model\n",
    "\n",
    "This notebook implements a discrete-time Markov Chain model for analyzing earthquake patterns in Indonesia using the Kaggle dataset.\n",
    "\n",
    "## Overview\n",
    "- **Goal**: Build a Markov Chain model to analyze earthquake state transitions\n",
    "- **Data**: Indonesian earthquake catalog (TSV format)\n",
    "- **Approach**: Create states based on magnitude and optionally location, then compute transition probabilities\n",
    "\n",
    "## Model Features\n",
    "1. **Magnitude-only states**: Small, Medium, Large earthquakes\n",
    "2. **Combined states**: Magnitude + coarse geographical regions\n",
    "3. **N-step transition analysis**: Compute multi-step transition probabilities\n",
    "4. **Exploratory analysis**: Summary statistics and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad86840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.1.3\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a7dd2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the data path and model parameters. **Edit the DATA_PATH variable to point to your local dataset file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3186fc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Data path: data/katalog_gempa_v2.tsv\n",
      "Magnitude thresholds: Small < 4.0, Medium < 6.0, Large >= 6.0\n",
      "Latitude bins: [-12, -6, 0, 6]\n",
      "Longitude bins: [90, 110, 130, 150]\n"
     ]
    }
   ],
   "source": [
    "# Configuration Section\n",
    "# ===================\n",
    "\n",
    "# Data path - EDIT THIS to point to your local dataset\n",
    "DATA_PATH = \"data/katalog_gempa_v2.tsv\"  # Update this path as needed\n",
    "\n",
    "# Magnitude thresholds for state definition\n",
    "MAG_THRESHOLD_MIN = 2.0  # Minimum magnitude to include\n",
    "MAG_SMALL_MAX = 4.0      # Small earthquakes: mag < 4.0\n",
    "MAG_MEDIUM_MAX = 6.0     # Medium earthquakes: 4.0 <= mag < 6.0\n",
    "                         # Large earthquakes: mag >= 6.0\n",
    "\n",
    "# Geographical bins for Indonesia (approximate bounds)\n",
    "# Indonesia roughly spans: lat [-12, 6], lon [90, 150]\n",
    "LAT_BINS = [-12, -6, 0, 6]      # Southern, Central, Northern regions\n",
    "LON_BINS = [90, 110, 130, 150]  # Western, Central, Eastern regions\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Magnitude thresholds: Small < {MAG_SMALL_MAX}, Medium < {MAG_MEDIUM_MAX}, Large >= {MAG_MEDIUM_MAX}\")\n",
    "print(f\"Latitude bins: {LAT_BINS}\")\n",
    "print(f\"Longitude bins: {LON_BINS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead1b22",
   "metadata": {},
   "source": [
    "## State Definition Functions\n",
    "\n",
    "Helper functions to classify earthquakes into discrete states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776860d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing state assignment functions:\n",
      "Magnitude 3.5 -> Small\n",
      "Magnitude 5.2 -> Medium\n",
      "Magnitude 7.1 -> Large\n",
      "Coordinates (-6.2, 106.8) -> R1\n",
      "Combined state example: R1_Medium\n"
     ]
    }
   ],
   "source": [
    "def assign_magnitude_state(mag: float) -> str:\n",
    "    \"\"\"\n",
    "    Classify earthquake magnitude into discrete states.\n",
    "    \n",
    "    Args:\n",
    "        mag (float): Earthquake magnitude\n",
    "        \n",
    "    Returns:\n",
    "        str: State label ('Small', 'Medium', 'Large')\n",
    "    \"\"\"\n",
    "    if pd.isna(mag):\n",
    "        return 'Unknown'\n",
    "    elif mag < MAG_SMALL_MAX:\n",
    "        return 'Small'\n",
    "    elif mag < MAG_MEDIUM_MAX:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "\n",
    "def assign_region_state(lat: float, lon: float) -> str:\n",
    "    \"\"\"\n",
    "    Assign geographical region based on latitude and longitude bins.\n",
    "    \n",
    "    Args:\n",
    "        lat (float): Latitude\n",
    "        lon (float): Longitude\n",
    "        \n",
    "    Returns:\n",
    "        str: Region identifier (e.g., 'R1', 'R2', ...)\n",
    "    \"\"\"\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Find latitude bin\n",
    "    lat_idx = np.digitize(lat, LAT_BINS) - 1\n",
    "    lat_idx = max(0, min(lat_idx, len(LAT_BINS) - 2))\n",
    "    \n",
    "    # Find longitude bin\n",
    "    lon_idx = np.digitize(lon, LON_BINS) - 1\n",
    "    lon_idx = max(0, min(lon_idx, len(LON_BINS) - 2))\n",
    "    \n",
    "    # Create region ID\n",
    "    region_id = lat_idx * (len(LON_BINS) - 1) + lon_idx + 1\n",
    "    return f'R{region_id}'\n",
    "\n",
    "\n",
    "def build_combined_state(row) -> str:\n",
    "    \"\"\"\n",
    "    Create combined state from magnitude and region.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with magnitude and coordinates\n",
    "        \n",
    "    Returns:\n",
    "        str: Combined state (e.g., 'R1_Small', 'R2_Large')\n",
    "    \"\"\"\n",
    "    mag_state = assign_magnitude_state(row['mag'])\n",
    "    region_state = assign_region_state(row['lat'], row['lon'])\n",
    "    \n",
    "    if mag_state == 'Unknown' or region_state == 'Unknown':\n",
    "        return 'Unknown'\n",
    "    \n",
    "    return f'{region_state}_{mag_state}'\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "print(\"Testing state assignment functions:\")\n",
    "print(f\"Magnitude 3.5 -> {assign_magnitude_state(3.5)}\")\n",
    "print(f\"Magnitude 5.2 -> {assign_magnitude_state(5.2)}\")\n",
    "print(f\"Magnitude 7.1 -> {assign_magnitude_state(7.1)}\")\n",
    "print(f\"Coordinates (-6.2, 106.8) -> {assign_region_state(-6.2, 106.8)}\")\n",
    "print(f\"Combined state example: {build_combined_state(pd.Series({'mag': 5.2, 'lat': -6.2, 'lon': 106.8}))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f6dd2",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "556afa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading function defined.\n",
      "Call load_and_preprocess_data(DATA_PATH) to load your earthquake data.\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load earthquake data from TSV file and perform preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the TSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and processed earthquake data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data with tab separator\n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        print(f\"Initial data shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Display sample of raw data\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Handle different possible column names\n",
    "        column_mapping = {}\n",
    "        \n",
    "        # Map date columns\n",
    "        if 'tgl' in df.columns:\n",
    "            column_mapping['date'] = 'tgl'\n",
    "        elif 'date' in df.columns:\n",
    "            column_mapping['date'] = 'date'\n",
    "        \n",
    "        # Map origin time columns  \n",
    "        if 'ot' in df.columns:\n",
    "            column_mapping['origin_time'] = 'ot'\n",
    "        elif 'origin_time' in df.columns:\n",
    "            column_mapping['origin_time'] = 'origin_time'\n",
    "        elif 'time' in df.columns:\n",
    "            column_mapping['origin_time'] = 'time'\n",
    "            \n",
    "        # Map coordinate columns\n",
    "        for coord in ['lat', 'lon', 'latitude', 'longitude']:\n",
    "            if coord in df.columns:\n",
    "                if coord.startswith('lat'):\n",
    "                    column_mapping['lat'] = coord\n",
    "                elif coord.startswith('lon'):\n",
    "                    column_mapping['lon'] = coord\n",
    "                    \n",
    "        # Map magnitude column\n",
    "        for mag_col in ['mag', 'magnitude', 'Magnitude']:\n",
    "            if mag_col in df.columns:\n",
    "                column_mapping['mag'] = mag_col\n",
    "                break\n",
    "                \n",
    "        # Map depth column if available\n",
    "        if 'depth' in df.columns:\n",
    "            column_mapping['depth'] = 'depth'\n",
    "        \n",
    "        print(f\"\\nColumn mapping: {column_mapping}\")\n",
    "        \n",
    "        # Rename columns to standard names\n",
    "        rename_dict = {v: k for k, v in column_mapping.items()}\n",
    "        df = df.rename(columns=rename_dict)\n",
    "        \n",
    "        # Create unified datetime column\n",
    "        if 'date' in df.columns and 'origin_time' in df.columns:\n",
    "            # Combine date and time\n",
    "            df['datetime_str'] = df['date'].astype(str) + ' ' + df['origin_time'].astype(str)\n",
    "            df['event_time'] = pd.to_datetime(df['datetime_str'], errors='coerce')\n",
    "        elif 'origin_time' in df.columns:\n",
    "            # Use origin_time directly\n",
    "            df['event_time'] = pd.to_datetime(df['origin_time'], errors='coerce')\n",
    "        elif 'date' in df.columns:\n",
    "            # Use date only\n",
    "            df['event_time'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        else:\n",
    "\n",
    "            raise ValueError(\"No suitable date/time columns found\")\n",
    "            \n",
    "        # Convert numeric columns\n",
    "        for col in ['lat', 'lon', 'mag', 'depth']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Remove rows with missing essential data\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['event_time', 'lat', 'lon', 'mag'])\n",
    "        df = df[df['mag'] >= MAG_THRESHOLD_MIN]  # Filter minimum magnitude\n",
    "        \n",
    "        print(f\"\\nData cleaning:\")\n",
    "        print(f\"Removed {initial_count - len(df)} rows with missing/invalid data\")\n",
    "        print(f\"Final data shape: {df.shape}\")\n",
    "        \n",
    "        # Sort by event time\n",
    "        df = df.sort_values('event_time').reset_index(drop=True)\n",
    "        \n",
    "        # Add state columns\n",
    "        df['state_mag'] = df['mag'].apply(assign_magnitude_state)\n",
    "        df['state_region_mag'] = df.apply(build_combined_state, axis=1)\n",
    "        \n",
    "        print(f\"\\nMagnitude distribution:\")\n",
    "        print(df['state_mag'].value_counts())\n",
    "        print(f\"\\nDate range: {df['event_time'].min()} to {df['event_time'].max()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please check the file path and format.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example of how to load data (will be used later)\n",
    "print(\"Data loading function defined.\")\n",
    "print(\"Call load_and_preprocess_data(DATA_PATH) to load your earthquake data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2c103",
   "metadata": {},
   "source": [
    "## Markov Chain Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12614352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov chain functions defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def compute_transition_matrix(states: list, state_order: list = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Build transition count and probability matrices from sequence of states.\n",
    "    \n",
    "    Args:\n",
    "        states (list): Time-ordered sequence of states\n",
    "        state_order (list): Fixed ordering of states for matrix indexing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (counts_array, probs_df, counts_df)\n",
    "            - counts_array: 2D numpy array of transition counts\n",
    "            - probs_df: DataFrame of transition probabilities  \n",
    "            - counts_df: DataFrame of transition counts\n",
    "    \"\"\"\n",
    "    # Remove any 'Unknown' states\n",
    "    clean_states = [s for s in states if s != 'Unknown']\n",
    "    \n",
    "    if len(clean_states) < 2:\n",
    "        raise ValueError(\"Need at least 2 valid states to build transition matrix\")\n",
    "    \n",
    "    # Determine state order\n",
    "    if state_order is None:\n",
    "        state_order = sorted(list(set(clean_states)))\n",
    "    \n",
    "    n_states = len(state_order)\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(state_order)}\n",
    "    \n",
    "    # Initialize count matrix\n",
    "    counts = np.zeros((n_states, n_states), dtype=int)\n",
    "    \n",
    "    # Count transitions\n",
    "    for i in range(len(clean_states) - 1):\n",
    "        current_state = clean_states[i]\n",
    "        next_state = clean_states[i + 1]\n",
    "        \n",
    "        if current_state in state_to_idx and next_state in state_to_idx:\n",
    "            current_idx = state_to_idx[current_state]\n",
    "            next_idx = state_to_idx[next_state]\n",
    "            counts[current_idx, next_idx] += 1\n",
    "    \n",
    "    # Convert to probability matrix\n",
    "    # Handle rows with zero transitions\n",
    "    probs = np.zeros_like(counts, dtype=float)\n",
    "    for i in range(n_states):\n",
    "        row_sum = counts[i, :].sum()\n",
    "        if row_sum > 0:\n",
    "            probs[i, :] = counts[i, :] / row_sum\n",
    "        else:\n",
    "            # If no transitions from this state, use uniform distribution\n",
    "            probs[i, :] = 1.0 / n_states\n",
    "    \n",
    "    # Create DataFrames for better visualization\n",
    "    counts_df = pd.DataFrame(counts, index=state_order, columns=state_order)\n",
    "    probs_df = pd.DataFrame(probs, index=state_order, columns=state_order)\n",
    "    \n",
    "    return counts, probs_df, counts_df\n",
    "\n",
    "\n",
    "def n_step_transition_matrix(P: np.ndarray, n: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute n-step transition matrix P^n.\n",
    "    \n",
    "    Args:\n",
    "        P (np.ndarray): One-step transition probability matrix\n",
    "        n (int): Number of steps\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: n-step transition matrix\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return np.eye(P.shape[0])\n",
    "    elif n == 1:\n",
    "        return P.copy()\n",
    "    else:\n",
    "        return np.linalg.matrix_power(P, n)\n",
    "\n",
    "\n",
    "def n_step_transition_probability(P: np.ndarray, state_order: list, \n",
    "                                 start_state: str, end_state: str, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute probability of transitioning from start_state to end_state in n steps.\n",
    "    \n",
    "    Args:\n",
    "        P (np.ndarray): One-step transition probability matrix\n",
    "        state_order (list): Ordered list of state names\n",
    "        start_state (str): Initial state\n",
    "        end_state (str): Target state  \n",
    "        n (int): Number of steps\n",
    "        \n",
    "    Returns:\n",
    "        float: Transition probability\n",
    "    \"\"\"\n",
    "    if start_state not in state_order:\n",
    "        raise ValueError(f\"Start state '{start_state}' not in state_order\")\n",
    "    if end_state not in state_order:\n",
    "        raise ValueError(f\"End state '{end_state}' not in state_order\")\n",
    "    \n",
    "    start_idx = state_order.index(start_state)\n",
    "    end_idx = state_order.index(end_state)\n",
    "    \n",
    "    P_n = n_step_transition_matrix(P, n)\n",
    "    return P_n[start_idx, end_idx]\n",
    "\n",
    "\n",
    "print(\"Markov chain functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641e47e",
   "metadata": {},
   "source": [
    "## Analysis and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac0a75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis functions defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def analyze_markov_chain(df: pd.DataFrame, state_column: str, analysis_name: str):\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis of a Markov chain.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Earthquake data\n",
    "        state_column (str): Column name containing states\n",
    "        analysis_name (str): Name for the analysis (for printing)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MARKOV CHAIN ANALYSIS: {analysis_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract state sequence\n",
    "    states = df[state_column].tolist()\n",
    "    clean_states = [s for s in states if s != 'Unknown']\n",
    "    \n",
    "    print(f\"Total events: {len(df)}\")\n",
    "    print(f\"Valid states for analysis: {len(clean_states)}\")\n",
    "    \n",
    "    if len(clean_states) < 2:\n",
    "        print(\"Insufficient data for Markov chain analysis.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Build transition matrices\n",
    "    try:\n",
    "        counts_array, probs_df, counts_df = compute_transition_matrix(clean_states)\n",
    "        state_order = probs_df.index.tolist()\n",
    "        \n",
    "        print(f\"\\nState distribution:\")\n",
    "        state_counts = pd.Series(clean_states).value_counts()\n",
    "        for state in state_order:\n",
    "            count = state_counts.get(state, 0)\n",
    "            pct = (count / len(clean_states)) * 100\n",
    "            print(f\"  {state}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTransition Count Matrix:\")\n",
    "        print(counts_df)\n",
    "        \n",
    "        print(f\"\\nTransition Probability Matrix:\")\n",
    "        print(probs_df.round(4))\n",
    "        \n",
    "        # Analyze interesting patterns\n",
    "        print(f\"\\nKey Transition Probabilities:\")\n",
    "        for from_state in state_order:\n",
    "            for to_state in state_order:\n",
    "                prob = probs_df.loc[from_state, to_state]\n",
    "                if prob > 0.1:  # Only show significant probabilities\n",
    "                    print(f\"  P({from_state} -> {to_state}) = {prob:.3f}\")\n",
    "        \n",
    "        # Find most likely transitions from each state\n",
    "        print(f\"\\nMost Likely Next State from Each State:\")\n",
    "        for state in state_order:\n",
    "            next_state = probs_df.loc[state].idxmax()\n",
    "            prob = probs_df.loc[state, next_state]\n",
    "            print(f\"  From {state}: most likely -> {next_state} (p={prob:.3f})\")\n",
    "        \n",
    "        # N-step analysis\n",
    "        print(f\"\\nMulti-Step Transition Analysis:\")\n",
    "        steps = [2, 5, 10]\n",
    "        \n",
    "        for n in steps:\n",
    "            print(f\"\\n{n}-step transitions:\")\n",
    "            P_n = n_step_transition_matrix(probs_df.values, n)\n",
    "            \n",
    "            # Show some example transitions\n",
    "            for i, from_state in enumerate(state_order[:3]):  # Limit to first 3 states\n",
    "                for j, to_state in enumerate(state_order):\n",
    "                    prob_n = P_n[i, j]\n",
    "                    if prob_n > 0.05:  # Only significant probabilities\n",
    "                        prob_1 = probs_df.iloc[i, j]\n",
    "                        print(f\"    P({from_state} -> {to_state} in {n} steps) = {prob_n:.3f} (1-step: {prob_1:.3f})\")\n",
    "        \n",
    "        return counts_array, probs_df, counts_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def print_model_assumptions():\n",
    "    \"\"\"\n",
    "    Print key assumptions and limitations of the Markov chain model.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL ASSUMPTIONS AND LIMITATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    ASSUMPTIONS:\n",
    "    1. First-order Markov property: Next state depends only on current state\n",
    "    2. Time-homogeneous: Transition probabilities are constant over time\n",
    "    3. Discrete states: Earthquakes classified into finite categories\n",
    "    4. Sequential dependence: Order of earthquakes matters\n",
    "    \n",
    "    LIMITATIONS:\n",
    "    1. No temporal spacing: Time between events is not modeled\n",
    "    2. No spatial correlation: Beyond coarse regional binning\n",
    "    3. No \"no-earthquake\" states: Only transitions between events\n",
    "    4. Assumes stationarity: Real seismic patterns may change over time\n",
    "    5. Limited by historical data: May not capture rare large events\n",
    "    \n",
    "    APPLICATIONS:\n",
    "    - Understanding general earthquake patterns\n",
    "    - Baseline for more complex models (HMM, PSHA)\n",
    "    - Educational tool for seismic sequence analysis\n",
    "    - Not suitable for precise earthquake prediction\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Analysis functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7b911",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "**Note**: Before running this section, make sure to:\n",
    "1. Download the dataset from: https://www.kaggle.com/datasets/kekavigi/earthquakes-in-indonesia\n",
    "2. Update the `DATA_PATH` variable above to point to your local file\n",
    "3. Ensure the file is in TSV (tab-separated) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3315d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO RUN ANALYSIS!\n",
      "\n",
      "To execute the complete analysis:\n",
      "1. Ensure DATA_PATH points to your earthquake dataset\n",
      "2. Run: results = main()\n",
      "\n",
      "The main() function will:\n",
      "- Load and preprocess the earthquake data\n",
      "- Build magnitude-only Markov chain\n",
      "- Build magnitude+region Markov chain  \n",
      "- Compute transition probabilities\n",
      "- Display multi-step transition analysis\n",
      "- Show model assumptions and limitations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main analysis pipeline for Indonesian earthquake Markov chain model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        print(\"Starting Indonesian Earthquake Markov Chain Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        df = load_and_preprocess_data(DATA_PATH)\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"Failed to load data. Please check the file path and format.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nSuccessfully loaded {len(df)} earthquake events\")\n",
    "        print(f\"Date range: {df['event_time'].min()} to {df['event_time'].max()}\")\n",
    "        \n",
    "        # Analyze magnitude-only Markov chain\n",
    "        print(\"\\n\\n\" + \"#\" * 80)\n",
    "        print(\"ANALYSIS 1: MAGNITUDE-ONLY STATES\")\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        mag_counts, mag_probs, mag_counts_df = analyze_markov_chain(\n",
    "            df, 'state_mag', 'Magnitude-Only States'\n",
    "        )\n",
    "        \n",
    "        # Analyze combined magnitude+region Markov chain\n",
    "        print(\"\\n\\n\" + \"#\" * 80)\n",
    "        print(\"ANALYSIS 2: MAGNITUDE + REGION STATES\")\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        region_counts, region_probs, region_counts_df = analyze_markov_chain(\n",
    "            df, 'state_region_mag', 'Magnitude + Region States'\n",
    "        )\n",
    "        \n",
    "        # Print model assumptions\n",
    "        print_model_assumptions()\n",
    "        \n",
    "        # Summary insights\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY INSIGHTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if mag_probs is not None:\n",
    "            # Find highest probability transitions\n",
    "            print(\"\\nHighest probability transitions (magnitude-only):\")\n",
    "            max_prob = 0\n",
    "            best_transition = None\n",
    "            \n",
    "            for i, from_state in enumerate(mag_probs.index):\n",
    "                for j, to_state in enumerate(mag_probs.columns):\n",
    "                    if i != j:  # Exclude self-transitions\n",
    "                        prob = mag_probs.iloc[i, j]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            best_transition = (from_state, to_state)\n",
    "            \n",
    "            if best_transition:\n",
    "                print(f\"  Strongest transition: {best_transition[0]} -> {best_transition[1]} (p={max_prob:.3f})\")\n",
    "            \n",
    "            # Analyze persistence\n",
    "            print(\"\\nState persistence (probability of staying in same state):\")\n",
    "            for state in mag_probs.index:\n",
    "                self_prob = mag_probs.loc[state, state]\n",
    "                print(f\"  P({state} -> {state}) = {self_prob:.3f}\")\n",
    "        \n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "        \n",
    "        # Return results for further analysis if needed\n",
    "        return {\n",
    "            'data': df,\n",
    "            'magnitude_analysis': (mag_counts, mag_probs, mag_counts_df),\n",
    "            'region_analysis': (region_counts, region_probs, region_counts_df)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Display instructions\n",
    "print(\"\"\"READY TO RUN ANALYSIS!\n",
    "\n",
    "To execute the complete analysis:\n",
    "1. Ensure DATA_PATH points to your earthquake dataset\n",
    "2. Run: results = main()\n",
    "\n",
    "The main() function will:\n",
    "- Load and preprocess the earthquake data\n",
    "- Build magnitude-only Markov chain\n",
    "- Build magnitude+region Markov chain  \n",
    "- Compute transition probabilities\n",
    "- Display multi-step transition analysis\n",
    "- Show model assumptions and limitations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e56a44",
   "metadata": {},
   "source": [
    "## Demo with Sample Data\n",
    "\n",
    "If you don't have the dataset yet, here's a demonstration with synthetic data to test the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89192944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING DEMONSTRATION WITH SYNTHETIC DATA\n",
      "============================================================\n",
      "Created 500 synthetic earthquake events\n",
      "\n",
      "Sample data:\n",
      "                     event_time       lat         lon       mag state_mag  \\\n",
      "0 2000-01-01 07:16:54.859989191 -8.667607  121.144907  3.396323     Small   \n",
      "1 2000-01-01 14:05:23.247829142 -2.245783  118.750913  7.072193     Large   \n",
      "2 2000-01-03 22:54:06.949601431  3.713025   91.538524  4.619055    Medium   \n",
      "3 2000-01-04 05:47:30.418693327  1.180048  110.474870  3.627590     Small   \n",
      "4 2000-01-04 13:24:32.497869849  2.518101  112.811737  3.369462     Small   \n",
      "5 2000-01-05 23:35:58.996580644 -0.141899  113.929367  2.325234     Small   \n",
      "6 2000-01-06 13:56:11.254722585  0.460978  124.810342  3.821854     Small   \n",
      "7 2000-01-08 01:55:09.599532116  3.285522  122.016153  5.645074    Medium   \n",
      "8 2000-01-08 03:32:30.277441146 -7.505976  126.474306  3.899600     Small   \n",
      "9 2000-01-08 19:35:16.755158909 -3.190351  135.892996  5.451439    Medium   \n",
      "\n",
      "  state_region_mag  \n",
      "0         R2_Small  \n",
      "1         R5_Large  \n",
      "2        R7_Medium  \n",
      "3         R8_Small  \n",
      "4         R8_Small  \n",
      "5         R5_Small  \n",
      "6         R8_Small  \n",
      "7        R8_Medium  \n",
      "8         R2_Small  \n",
      "9        R6_Medium  \n",
      "\n",
      "============================================================\n",
      "MARKOV CHAIN ANALYSIS: DEMO: Magnitude States\n",
      "============================================================\n",
      "Total events: 500\n",
      "Valid states for analysis: 500\n",
      "\n",
      "State distribution:\n",
      "  Large: 30 (6.0%)\n",
      "  Medium: 124 (24.8%)\n",
      "  Small: 346 (69.2%)\n",
      "\n",
      "Transition Count Matrix:\n",
      "        Large  Medium  Small\n",
      "Large       4      12     13\n",
      "Medium      5      31     88\n",
      "Small      21      81    244\n",
      "\n",
      "Transition Probability Matrix:\n",
      "         Large  Medium   Small\n",
      "Large   0.1379  0.4138  0.4483\n",
      "Medium  0.0403  0.2500  0.7097\n",
      "Small   0.0607  0.2341  0.7052\n",
      "\n",
      "Key Transition Probabilities:\n",
      "  P(Large -> Large) = 0.138\n",
      "  P(Large -> Medium) = 0.414\n",
      "  P(Large -> Small) = 0.448\n",
      "  P(Medium -> Medium) = 0.250\n",
      "  P(Medium -> Small) = 0.710\n",
      "  P(Small -> Medium) = 0.234\n",
      "  P(Small -> Small) = 0.705\n",
      "\n",
      "Most Likely Next State from Each State:\n",
      "  From Large: most likely -> Small (p=0.448)\n",
      "  From Medium: most likely -> Small (p=0.710)\n",
      "  From Small: most likely -> Small (p=0.705)\n",
      "\n",
      "Multi-Step Transition Analysis:\n",
      "\n",
      "2-step transitions:\n",
      "    P(Large -> Large in 2 steps) = 0.063 (1-step: 0.138)\n",
      "    P(Large -> Medium in 2 steps) = 0.265 (1-step: 0.414)\n",
      "    P(Large -> Small in 2 steps) = 0.672 (1-step: 0.448)\n",
      "    P(Medium -> Large in 2 steps) = 0.059 (1-step: 0.040)\n",
      "    P(Medium -> Medium in 2 steps) = 0.245 (1-step: 0.250)\n",
      "    P(Medium -> Small in 2 steps) = 0.696 (1-step: 0.710)\n",
      "    P(Small -> Large in 2 steps) = 0.061 (1-step: 0.061)\n",
      "    P(Small -> Medium in 2 steps) = 0.249 (1-step: 0.234)\n",
      "    P(Small -> Small in 2 steps) = 0.691 (1-step: 0.705)\n",
      "\n",
      "5-step transitions:\n",
      "    P(Large -> Large in 5 steps) = 0.060 (1-step: 0.138)\n",
      "    P(Large -> Medium in 5 steps) = 0.249 (1-step: 0.414)\n",
      "    P(Large -> Small in 5 steps) = 0.691 (1-step: 0.448)\n",
      "    P(Medium -> Large in 5 steps) = 0.060 (1-step: 0.040)\n",
      "    P(Medium -> Medium in 5 steps) = 0.249 (1-step: 0.250)\n",
      "    P(Medium -> Small in 5 steps) = 0.691 (1-step: 0.710)\n",
      "    P(Small -> Large in 5 steps) = 0.060 (1-step: 0.061)\n",
      "    P(Small -> Medium in 5 steps) = 0.249 (1-step: 0.234)\n",
      "    P(Small -> Small in 5 steps) = 0.691 (1-step: 0.705)\n",
      "\n",
      "10-step transitions:\n",
      "    P(Large -> Large in 10 steps) = 0.060 (1-step: 0.138)\n",
      "    P(Large -> Medium in 10 steps) = 0.249 (1-step: 0.414)\n",
      "    P(Large -> Small in 10 steps) = 0.691 (1-step: 0.448)\n",
      "    P(Medium -> Large in 10 steps) = 0.060 (1-step: 0.040)\n",
      "    P(Medium -> Medium in 10 steps) = 0.249 (1-step: 0.250)\n",
      "    P(Medium -> Small in 10 steps) = 0.691 (1-step: 0.710)\n",
      "    P(Small -> Large in 10 steps) = 0.060 (1-step: 0.061)\n",
      "    P(Small -> Medium in 10 steps) = 0.249 (1-step: 0.234)\n",
      "    P(Small -> Small in 10 steps) = 0.691 (1-step: 0.705)\n",
      "\n",
      "Testing n-step transition functions:\n",
      "P(Small -> Large in 1 steps) = 0.0607\n",
      "P(Small -> Large in 3 steps) = 0.0603\n",
      "P(Small -> Large in 5 steps) = 0.0603\n",
      "\n",
      "Demo completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_sample_data(n_events: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic earthquake data for testing purposes.\n",
    "    \n",
    "    Args:\n",
    "        n_events (int): Number of synthetic events to generate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Synthetic earthquake data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Generate synthetic data with realistic patterns\n",
    "    # Small earthquakes are most common, large ones are rare\n",
    "    magnitude_probs = [0.7, 0.25, 0.05]  # Small, Medium, Large\n",
    "    magnitudes = np.random.choice(['Small', 'Medium', 'Large'], \n",
    "                                  n_events, p=magnitude_probs)\n",
    "    \n",
    "    # Convert to numeric magnitudes\n",
    "    mag_mapping = {'Small': (2.0, 4.0), 'Medium': (4.0, 6.0), 'Large': (6.0, 8.0)}\n",
    "    numeric_mags = []\n",
    "    for mag_type in magnitudes:\n",
    "        min_mag, max_mag = mag_mapping[mag_type]\n",
    "        numeric_mags.append(np.random.uniform(min_mag, max_mag))\n",
    "    \n",
    "    # Generate coordinates within Indonesia bounds\n",
    "    lats = np.random.uniform(-12, 6, n_events)\n",
    "    lons = np.random.uniform(90, 150, n_events)\n",
    "    \n",
    "    # Generate time sequence\n",
    "    start_date = pd.Timestamp('2000-01-01')\n",
    "    time_deltas = np.random.exponential(1, n_events)  # Random intervals\n",
    "    event_times = [start_date + pd.Timedelta(days=sum(time_deltas[:i+1])) \n",
    "                   for i in range(n_events)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'event_time': event_times,\n",
    "        'lat': lats,\n",
    "        'lon': lons,\n",
    "        'mag': numeric_mags,\n",
    "        'depth': np.random.uniform(10, 200, n_events)\n",
    "    })\n",
    "    \n",
    "    # Add state columns\n",
    "    df['state_mag'] = df['mag'].apply(assign_magnitude_state)\n",
    "    df['state_region_mag'] = df.apply(build_combined_state, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run_demo():\n",
    "    \"\"\"\n",
    "    Run a demonstration with synthetic data.\n",
    "    \"\"\"\n",
    "    print(\"RUNNING DEMONSTRATION WITH SYNTHETIC DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create sample data\n",
    "    demo_df = create_sample_data(500)  # 500 events for demo\n",
    "    \n",
    "    print(f\"Created {len(demo_df)} synthetic earthquake events\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(demo_df[['event_time', 'lat', 'lon', 'mag', 'state_mag', 'state_region_mag']].head(10))\n",
    "    \n",
    "    # Run analysis on synthetic data\n",
    "    mag_counts, mag_probs, mag_counts_df = analyze_markov_chain(\n",
    "        demo_df, 'state_mag', 'DEMO: Magnitude States'\n",
    "    )\n",
    "    \n",
    "    if mag_probs is not None:\n",
    "        # Test n-step transitions\n",
    "        print(\"\\nTesting n-step transition functions:\")\n",
    "        state_order = mag_probs.index.tolist()\n",
    "        \n",
    "        # Example: probability of going from Small to Large in various steps\n",
    "        for n in [1, 3, 5]:\n",
    "            try:\n",
    "                prob = n_step_transition_probability(\n",
    "                    mag_probs.values, state_order, 'Small', 'Large', n\n",
    "                )\n",
    "                print(f\"P(Small -> Large in {n} steps) = {prob:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing {n}-step transition: {e}\")\n",
    "    \n",
    "    print(\"\\nDemo completed successfully!\")\n",
    "    return demo_df\n",
    "\n",
    "# Run the demo\n",
    "demo_results = run_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199b2f1",
   "metadata": {},
   "source": [
    "## Run Analysis on Real Data\n",
    "\n",
    "Once you have downloaded and set up the real earthquake dataset, run this cell to perform the complete analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b681c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 11); perhaps you escaped the end quote? (2090936694.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Ready to analyze real earthquake data!\\\")\\nprint(\\\"Steps to run real analysis:\\\")\\nprint(\\\"1. Download dataset from: https://www.kaggle.com/datasets/kekavigi/earthquakes-in-indonesia\\\")\\nprint(\\\"2. Update DATA_PATH variable to point to your local file\\\")\\nprint(\\\"3. Uncomment and run: results = main()\\\")\\nprint(\\\"\\\\nAlternatively, test individual functions:\\\")\\nprint(\\\"- load_and_preprocess_data(DATA_PATH)\\\")\\nprint(\\\"- analyze_markov_chain(df, 'state_mag', 'Magnitude Analysis')\\\")\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 11); perhaps you escaped the end quote?\n"
     ]
    }
   ],
   "source": [
    "# Execute the complete analysis on real earthquake data\n",
    "# Uncomment and run this when you have the real dataset\n",
    "\n",
    "# results = main()\n",
    "\n",
    "# If successful, results will contain:\n",
    "# - results['data']: processed DataFrame\n",
    "# - results['magnitude_analysis']: transition matrices for magnitude-only states  \n",
    "# - results['region_analysis']: transition matrices for magnitude+region states\n",
    "\n",
    "print(\\\"Ready to analyze real earthquake data!\\\")\\nprint(\\\"Steps to run real analysis:\\\")\\nprint(\\\"1. Download dataset from: https://www.kaggle.com/datasets/kekavigi/earthquakes-in-indonesia\\\")\\nprint(\\\"2. Update DATA_PATH variable to point to your local file\\\")\\nprint(\\\"3. Uncomment and run: results = main()\\\")\\nprint(\\\"\\\\nAlternatively, test individual functions:\\\")\\nprint(\\\"- load_and_preprocess_data(DATA_PATH)\\\")\\nprint(\\\"- analyze_markov_chain(df, 'state_mag', 'Magnitude Analysis')\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20aeb5",
   "metadata": {},
   "source": [
    "## Additional Utilities\n",
    "\n",
    "Helpful functions for further analysis and exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e079330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional utility functions defined:\n",
      "- save_results_to_file(results, filename)\n",
      "- compute_steady_state(P)\n",
      "- analyze_steady_state(probs_df, state_name)\n",
      "\n",
      "Example usage after running main():\n",
      "# results = main()\n",
      "# save_results_to_file(results)\n",
      "# analyze_steady_state(results['magnitude_analysis'][1], 'Magnitude States')\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_file(results: dict, filename: str = \"markov_analysis_results.txt\"):\n",
    "    \"\"\"\n",
    "    Save analysis results to a text file for future reference.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results from main() function\n",
    "        filename (str): Output filename\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Indonesian Earthquake Markov Chain Analysis Results\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        df = results['data']\n",
    "        f.write(f\"Dataset Summary:\\n\")\n",
    "        f.write(f\"Total events: {len(df)}\\n\")\n",
    "        f.write(f\"Date range: {df['event_time'].min()} to {df['event_time'].max()}\\n\\n\")\n",
    "        \n",
    "        # Magnitude analysis\n",
    "        if results['magnitude_analysis'][1] is not None:\n",
    "            _, mag_probs, _ = results['magnitude_analysis']\n",
    "            f.write(\"Magnitude-Only State Transition Probabilities:\\n\")\n",
    "            f.write(str(mag_probs.round(4)))\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Region analysis  \n",
    "        if results['region_analysis'][1] is not None:\n",
    "            _, region_probs, _ = results['region_analysis']\n",
    "            f.write(\"Magnitude+Region State Transition Probabilities:\\n\")\n",
    "            f.write(str(region_probs.round(4)))\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "\n",
    "def compute_steady_state(P: np.ndarray, tolerance: float = 1e-10, max_iterations: int = 1000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the steady-state distribution of a Markov chain.\n",
    "    \n",
    "    Args:\n",
    "        P (np.ndarray): Transition probability matrix\n",
    "        tolerance (float): Convergence tolerance\n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Steady-state probability distribution\n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    # Start with uniform distribution\n",
    "    pi = np.ones(n) / n\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        pi_new = pi @ P\n",
    "        if np.allclose(pi, pi_new, atol=tolerance):\n",
    "            return pi_new\n",
    "        pi = pi_new\n",
    "    \n",
    "    print(f\"Warning: Steady state did not converge after {max_iterations} iterations\")\n",
    "    return pi\n",
    "\n",
    "\n",
    "def analyze_steady_state(probs_df: pd.DataFrame, state_name: str):\n",
    "    \"\"\"\n",
    "    Analyze the steady-state distribution of a Markov chain.\n",
    "    \n",
    "    Args:\n",
    "        probs_df (pd.DataFrame): Transition probability matrix\n",
    "        state_name (str): Name of the state system for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\nSteady-State Analysis for {state_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    steady_state = compute_steady_state(probs_df.values)\n",
    "    \n",
    "    print(\"Long-run state probabilities:\")\n",
    "    for i, state in enumerate(probs_df.index):\n",
    "        print(f\"  Ï€({state}) = {steady_state[i]:.4f}\")\n",
    "    \n",
    "    # Find most probable long-term state\n",
    "    max_idx = np.argmax(steady_state)\n",
    "    max_state = probs_df.index[max_idx]\n",
    "    print(f\"\\nMost probable long-term state: {max_state} ({steady_state[max_idx]:.4f})\")\n",
    "\n",
    "\n",
    "# Example usage functions\n",
    "print(\"Additional utility functions defined:\")\n",
    "print(\"- save_results_to_file(results, filename)\")\n",
    "print(\"- compute_steady_state(P)\")\n",
    "print(\"- analyze_steady_state(probs_df, state_name)\")\n",
    "print(\"\\nExample usage after running main():\")\n",
    "print(\"# results = main()\")\n",
    "print(\"# save_results_to_file(results)\")\n",
    "print(\"# analyze_steady_state(results['magnitude_analysis'][1], 'Magnitude States')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
